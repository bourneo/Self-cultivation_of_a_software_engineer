
分布式锁的三种实现方式

	Zookeeper；
	缓存 (推荐)；
	数据库。
	
	
	
	
	
	
	
	
	
	
	
	

分布式锁的几种实现方式
	
	目前几乎很多大型网站及应用都是分布式部署的，分布式场景中的数据一致性问题一直是一个比较重要的话题。
	分布式的 CAP 理论告诉我们 “任何一个分布式系统都无法同时满足
	一致性 (Consistency) 、可用性 (Availability) 和分区容错性 (Partition tolerance) ，最多只能同时满足两项。”
	所以，很多系统在设计之初就要对这三者做出取舍。
	在互联网领域的绝大多数的场景中，都需要牺牲强一致性来换取系统的高可用性，系统往往只需要保证“最终一致性”，
	只要这个最终时间是在用户可以接受的范围内即可。

	在很多场景中，我们为了保证数据的最终一致性，需要很多的技术方案来支持，比如分布式事务、分布式锁等。
	有的时候，我们需要保证一个方法在同一时间内只能被同一个线程执行。
	在单机环境中，Java 中其实提供了很多并发处理相关的 API，但是这些 API 在分布式场景中就无能为力了。
	也就是说单纯的 Java Api 并不能提供分布式锁的能力。所以针对分布式锁的实现目前有多种方案。

	针对分布式锁的实现，目前比较常用的有以下几种方案：

		基于数据库实现分布式锁；
		基于缓存 (redis，memcached，tair) 实现分布式锁；
		基于 Zookeeper 实现分布式锁。

	在分析这几种实现方案之前我们先来想一下，我们需要的分布式锁应该是怎么样的？ (这里以方法锁为例，资源锁同理) 

		可以保证在分布式部署的应用集群中，同一个方法在同一时间只能被一台机器上的一个线程执行。
		这把锁要是一把可重入锁 (避免死锁) 
		这把锁最好是一把阻塞锁 (根据业务需求考虑要不要这条) 
		有高可用的获取锁和释放锁功能
		获取锁和释放锁的性能要好
	
	
	
	基于数据库实现分布式锁
		
		基于数据库表
		
			要实现分布式锁，最简单的方式可能就是直接创建一张锁表，然后通过操作该表中的数据来实现了。

			当我们要锁住某个方法或资源时，我们就在该表中增加一条记录，想要释放锁的时候就删除这条记录。

			创建这样一张数据库表：

			当我们想要锁住某个方法时，执行以下 SQL：

			因为我们对 method_name 做了唯一性约束，这里如果有多个请求同时提交到数据库的话，数据库会保证只有一个操作可以成功，那么我们就可以认为操作成功的那个线程获得了该方法的锁，可以执行方法体内容。

			当方法执行完毕之后，想要释放锁的话，需要执行以下 Sql:



			上面这种简单的实现有以下几个问题：

				1、这把锁强依赖数据库的可用性，数据库是一个单点，一旦数据库挂掉，会导致业务系统不可用。

				2、这把锁没有失效时间，一旦解锁操作失败，就会导致锁记录一直在数据库中，其他线程无法再获得到锁。

				3、这把锁只能是非阻塞的，因为数据的 insert 操作，一旦插入失败就会直接报错。
					没有获得锁的线程并不会进入排队队列，要想再次获得锁就要再次触发获得锁操作。

				4、这把锁是非重入的，同一个线程在没有释放锁之前无法再次获得该锁。因为数据中数据已经存在了。

			当然，我们也可以有其他方式解决上面的问题。

			数据库是单点？搞两个数据库，数据之前双向同步。一旦挂掉快速切换到备库上。
			没有失效时间？只要做一个定时任务，每隔一定时间把数据库中的超时数据清理一遍。
			非阻塞的？搞一个 while 循环，直到 insert 成功再返回成功。
			非重入的？在数据库表中加个字段，记录当前获得锁的机器的主机信息和线程信息，那么下次再获取锁的时候先查询数据库，如果当前机器的主机信息和线程信息在数据库可以查到的话，直接把锁分配给他就可以了。
		
		
		基于数据库排他锁
			
			除了可以通过增删操作数据表中的记录以外，其实还可以借助数据中自带的锁来实现分布式的锁。

			我们还用刚刚创建的那张数据库表。可以通过数据库的排他锁来实现分布式锁。基于 MySql 的 InnoDB 引擎，可以使用以下方法来实现加锁操作：



			在查询语句后面增加 for update，数据库会在查询过程中给数据库表增加排他锁 (这里再多提一句，InnoDB 引擎在加锁的时候，只有通过索引进行检索的时候才会使用行级锁，否则会使用表级锁。这里我们希望使用行级锁，就要给 method_name 添加索引，值得注意的是，这个索引一定要创建成唯一索引，否则会出现多个重载方法之间无法同时被访问的问题。重载方法的话建议把参数类型也加上。) 。当某条记录被加上排他锁之后，其他线程无法再在该行记录上增加排他锁。

			我们可以认为获得排它锁的线程即可获得分布式锁，当获取到锁之后，可以执行方法的业务逻辑，执行完方法之后，再通过以下方法解锁：



			通过 connection.commit() 操作来释放锁。

			这种方法可以有效的解决上面提到的无法释放锁和阻塞锁的问题。

			阻塞锁？for update 语句会在执行成功后立即返回，在执行失败时一直处于阻塞状态，直到成功。
			锁定之后服务宕机，无法释放？使用这种方式，服务宕机之后数据库会自己把锁释放掉。
			但是还是无法直接解决数据库单点和可重入问题。

			这里还可能存在另外一个问题，虽然我们对 method_name 使用了唯一索引，并且显示使用 for update 来使用行级锁。但是，MySql 会对查询进行优化，即便在条件中使用了索引字段，但是否使用索引来检索数据是由 MySQL 通过判断不同执行计划的代价来决定的，如果 MySQL 认为全表扫效率更高，比如对一些很小的表，它就不会使用索引，这种情况下 InnoDB 将使用表锁，而不是行锁。如果发生这种情况就悲剧了。。。

			还有一个问题，就是我们要使用排他锁来进行分布式锁的 lock，那么一个排他锁长时间不提交，就会占用数据库连接。一旦类似的连接变得多了，就可能把数据库连接池撑爆

		总结
			总结一下使用数据库来实现分布式锁的方式，这两种方式都是依赖数据库的一张表，一种是通过表中的记录的存在情况确定当前是否有锁存在，另外一种是通过数据库的排他锁来实现分布式锁。

			数据库实现分布式锁的优点
			直接借助数据库，容易理解。
			数据库实现分布式锁的缺点
			会有各种各样的问题，在解决问题的过程中会使整个方案变得越来越复杂。
			操作数据库需要一定的开销，性能问题需要考虑。
			使用数据库的行级锁并不一定靠谱，尤其是当我们的锁表并不大的时候。

	基于缓存实现分布式锁
		相比较于基于数据库实现分布式锁的方案来说，基于缓存来实现在性能方面会表现的更好一点。
		而且很多缓存是可以集群部署的，可以解决单点问题。
		
		目前有很多成熟的缓存产品，包括 Redis，memcached 以及我们公司内部的 Tair。
		这里以 Tair 为例来分析下使用缓存实现分布式锁的方案。
		关于 Redis 和 memcached 在网络上有很多相关的文章，并且也有一些成熟的框架及算法可以直接使用。
		基于 Tair 的实现分布式锁其实和 Redis 类似，其中主要的实现方式是使用 TairManager.put 方法来实现。

		以上实现方式同样存在几个问题：

			1、这把锁没有失效时间，一旦解锁操作失败，就会导致锁记录一直在 tair 中，其他线程无法再获得到锁。
			2、这把锁只能是非阻塞的，无论成功还是失败都直接返回。
			3、这把锁是非重入的，一个线程获得锁之后，在释放锁之前，无法再次获得该锁，
				因为使用到的 key 在 tair 中已经存在。无法再执行 put 操作。

		当然，同样有方式可以解决。

			没有失效时间？tair 的 put 方法支持传入失效时间，到达时间之后数据会自动删除。
			非阻塞？while 重复执行。
			非可重入？在一个线程获取到锁之后，把当前主机信息和线程信息保存起来，下次再获取之前先检查自己是不是当前锁的拥有者。
		但是，失效时间我设置多长时间为好？如何设置的失效时间太短，方法没等执行完，锁就自动释放了，那么就会产生并发问题。如果设置的时间太长，其他获取锁的线程就可能要平白的多等一段时间。这个问题使用数据库实现分布式锁同样存在

	总结
		可以使用缓存来代替数据库来实现分布式锁，这个可以提供更好的性能，同时，很多缓存服务都是集群部署的，可以避免单点问题。并且很多缓存服务都提供了可以用来实现分布式锁的方法，比如 Tair 的 put 方法，redis 的 setnx 方法等。并且，这些缓存服务也都提供了对数据的过期自动删除的支持，可以直接设置超时时间来控制锁的释放。
		
		使用缓存实现分布式锁的优点
		性能好，实现起来较为方便。
		使用缓存实现分布式锁的缺点
		通过超时时间来控制锁的失效时间并不是十分的靠谱。
		
		
	基于 Zookeeper 实现分布式锁
		
		基于 zookeeper 临时有序节点可以实现的分布式锁。
		大致思想即为：
			每个客户端对某个方法加锁时，在 zookeeper 上的与该方法对应的指定节点的目录下，生成一个唯一的瞬时有序节点。
		判断是否获取锁的方式很简单，只需要判断有序节点中序号最小的一个。
		当释放锁的时候，只需将这个瞬时节点删除即可。
		同时，其可以避免服务宕机导致的锁无法释放，而产生的死锁问题。

		来看下 Zookeeper 能不能解决前面提到的问题。
			锁无法释放？
			使用 Zookeeper 可以有效的解决锁无法释放的问题，
			因为在创建锁的时候，客户端会在 ZK 中创建一个临时节点，
			一旦客户端获取到锁之后突然挂掉 (Session 连接断开) ，那么这个临时节点就会自动删除掉。
			其他客户端就可以再次获得锁。

			非阻塞锁？
			使用 Zookeeper 可以实现阻塞的锁，
			客户端可以通过在 ZK 中创建顺序节点，并且在节点上绑定监听器，
			一旦节点有变化，Zookeeper 会通知客户端，客户端可以检查自己创建的节点是不是当前所有节点中序号最小的，
			如果是，那么自己就获取到锁，便可以执行业务逻辑了。

			不可重入？
			使用 Zookeeper 也可以有效的解决不可重入的问题，客户端在创建节点的时候，
			把当前客户端的主机信息和线程信息直接写入到节点中，
			下次想要获取锁的时候和当前最小的节点中的数据比对一下就可以了。
			如果和自己的信息一样，那么自己直接获取到锁，如果不一样就再创建一个临时的顺序节点，参与排队。

			单点问题？
			使用 Zookeeper 可以有效的解决单点问题，ZK 是集群部署的，
			只要集群中有半数以上的机器存活，就可以对外提供服务。
		
		可以直接使用 zookeeper 第三方库 Curator 客户端，这个客户端中封装了一个可重入的锁服务。
		
		Curator 提供的 InterProcessMutex 是分布式锁的实现。acquire 方法用户获取锁，release 方法用于释放锁。

		使用 ZK 实现的分布式锁好像完全符合了本文开头我们对一个分布式锁的所有期望。
		但是，其实并不是，Zookeeper 实现的分布式锁其实存在一个缺点，那就是性能上可能并没有缓存服务那么高。
		因为每次在创建锁和释放锁的过程中，都要动态创建、销毁瞬时节点来实现锁功能。
		ZK 中创建和删除节点只能通过 Leader 服务器来执行，然后将数据同不到所有的 Follower 机器上。
		
		其实，使用 Zookeeper 也有可能带来并发问题，只是并不常见而已。
		考虑这样的情况，由于网络抖动，客户端可 ZK 集群的 session 连接断了，
		那么 zk 以为客户端挂了，就会删除临时节点，这时候其他客户端就可以获取到分布式锁了。
		就可能产生并发问题。这个问题不常见是因为 zk 有重试机制，
		一旦 zk 集群检测不到客户端的心跳，就会重试，Curator 客户端支持多种重试策略。
		多次重试之后还不行的话才会删除临时节点。 (所以，选择一个合适的重试策略也比较重要，要在锁的粒度和并发之间找一个平衡。) 
	
	总结
		使用 Zookeeper 实现分布式锁的优点：
			有效的解决单点问题，不可重入问题，非阻塞问题以及锁无法释放的问题。实现起来较为简单。
		使用 Zookeeper 实现分布式锁的缺点：
			性能上不如使用缓存实现分布式锁。需要对 ZK 的原理有所了解。

	三种方案的比较
		上面几种方式，哪种方式都无法做到完美。
		就像 CAP 一样，在复杂性、可靠性、性能等方面无法同时满足，
			所以，根据不同的应用场景选择最适合自己的才是王道。

		从理解的难易程度角度 (从低到高) 
			数据库 > 缓存 > Zookeeper

		从实现的复杂性角度 (从低到高) 
			Zookeeper >= 缓存 > 数据库

		从性能角度 (从高到低) 
			缓存 > Zookeeper >= 数据库

		从可靠性角度 (从高到低) 
			Zookeeper > 缓存 > 数据库



分布式锁的三种实现方式

	一、zookeeper

		1、实现原理：

			基于 zookeeper 瞬时有序节点实现的分布式锁，其主要逻辑如下 (该图来自于 IBM 网站) 。
			大致思想即为：
			每个客户端对某个功能加锁时，在 zookeeper 上的与该功能对应的指定节点的目录下，生成一个唯一的瞬时有序节点。
			判断是否获取锁的方式很简单，只需要判断有序节点中序号最小的一个。
			当释放锁的时候，只需将这个瞬时节点删除即可。同时，其可以避免服务宕机导致的锁无法释放，而产生的死锁问题。

		2、优点

			锁安全性高，zk 可持久化

		3、缺点

			性能开销比较高。因为其需要动态产生、销毁瞬时节点来实现锁功能。

		4、实现

			可以直接采用 zookeeper 第三方库 curator 即可方便地实现分布式锁。
			以下为基于 curator 实现的 zk 分布式锁核心代码：
		
				@Override 
				public boolean tryLock(LockInfo info) { 
				 InterProcessMutex mutex = getMutex(info); 
				 int tryTimes = info.getTryTimes(); 
				 long tryInterval = info.getTryInterval(); 
				 boolean flag = true;// 代表是否需要重试 
				 while (flag && --tryTimes >= 0) { 
					try { 
					 if (mutex.acquire(info.getWaitLockTime(), TimeUnit.MILLISECONDS)) { 
						LOGGER.info(LogConstant.DST_LOCK + "acquire lock successfully!"); 
						flag = false; 
						break; 
					 } 
					} catch (Exception e) { 
					 LOGGER.error(LogConstant.DST_LOCK + "acquire lock error!", e); 
					} finally { 
					 checkAndRetry(flag, tryInterval, tryTimes); 
					} 
				 } 
				 return !flag;// 最后还需要重试，说明没拿到锁 
				} 
				 
				@Override 
				public boolean releaseLock(LockInfo info) { 
				 InterProcessMutex mutex = getMutex(info); 
				 int tryTimes = info.getTryTimes(); 
				 long tryInterval = info.getTryInterval(); 
				 boolean flag = true;// 代表是否需要重试 
				 while (flag && --tryTimes >= 0) { 
					try { 
					 mutex.release(); 
					 LOGGER.info(LogConstant.DST_LOCK + "release lock successfully!"); 
					 flag = false; 
					 break; 
					} catch (Exception e) { 
					 LOGGER.error(LogConstant.DST_LOCK + "release lock error!", e); 
					} finally { 
					 checkAndRetry(flag, tryInterval, tryTimes); 
					} 
				 } 
				 return !flag;// 最后还需要重试，说明没拿到锁 
				} 
				 
				/** 
				 * 获取锁。此处需要加同步，concurrentHashmap 无法避免此处的同步问题 
				 * @param info 锁信息 
				 * @return 锁实例 
				 */ 
				 private synchronized InterProcessMutex getMutex(LockInfo info) { 
					InterProcessReadWriteLock lock = null; 
					if (locksCache.get(info.getLock()) != null) { 
					 lock = locksCache.get(info.getLock()); 
					} else { 
					 lock = new InterProcessReadWriteLock(client, BASE_DIR + info.getLock()); 
					 locksCache.put(info.getLock(), lock); 
					} 
					InterProcessMutex mutex = null; 
					switch (info.getIsolate()) { 
					case READ: 
					 mutex = lock.readLock(); 
					 break; 
					case WRITE: 
					 mutex = lock.writeLock(); 
					 break; 
					default: 
					 throw new IllegalArgumentException(); 
					} 
					return mutex; 
				 } 
				 
				/** 
				 * 判断是否需要重试 
				 * @param flag 是否需要重试标志 
				 * @param tryInterval 重试间隔 
				 * @param tryTimes 重试次数 
				 */ 
				private void checkAndRetry(boolean flag, long tryInterval, int tryTimes) { 
				 try { 
					if (flag) { 
					 Thread.sleep(tryInterval); 
					 LOGGER.info(LogConstant.DST_LOCK + "retry getting lock! now retry time left: " + tryTimes); 
					} 
				 } catch (InterruptedException e) { 
					LOGGER.error(LogConstant.DST_LOCK + "retry interval thread interruptted!", e); 
				 } 
				} 
	 
	二、memcached 分布式锁

		1、实现原理：

			memcached 带有 add 函数，利用 add 函数的特性即可实现分布式锁。
			add 和 set 的区别在于：如果多线程并发 set，则每个 set 都会成功，但最后存储的值以最后的 set 的线程为准。
			而 add 的话则相反，add 会添加第一个到达的值，并返回 true，后续的添加则都会返回 false。
			利用该点即可很轻松地实现分布式锁。

		2、优点

			并发高效。

		3、缺点

			1) memcached 采用列入 LRU 置换策略，所以如果内存不够，可能导致缓存中的锁信息丢失。

			2) memcached 无法持久化，一旦重启，将导致信息丢失。

	三、redis 分布式锁

		redis 分布式锁即可以结合 zk 分布式锁锁高度安全和 memcached 并发场景下效率很好的优点，可以利用 jedis 客户端实现。



电商课题 V：分布式锁

	电商目的：
		保证整个 (分布式) 系统内对一个重要事物 (订单，账户等) 的有效操作线程，同一时间内有且只有一个。
		比如交易中心有 N 台服务器，订单中心有 M 台服务器，
		如何保证一个订单的同一笔支付处理，一个账户的同一笔充值操作是原子性的。
	 
	基于哪些服务实现分布式锁？
		memcache
		ZooKeeper
		Redis
		Hazelcast
		google Chubby

	基于 memcache 的分布式锁
		
		memcache 的所有命令都是原子性的 (internally atomic) ，所以利用它的 add 命令即可。
			简单伪码：
				if (cache.add("lock:{orderid}", currenttimestamp, expiredtime)) {
					// 已获得锁，继续
					try { do something } catch { ... }
					cache.delete("lock.{orderid}")
				} else { 
					// 或等待锁超时，或重试，或返回
				}
	 
		上面代码所暴露的常见性问题
			1) 如持有锁的线程异常退出或宕机，锁并没有释放；
			2) 设置了 key 的 expire，那么如果有新线程在 key 过期后拿到了新的锁，原来超时的线程回来时，
				如果不经判断会误认为那是它持有的锁，会误删锁。
	 
		解决思路
			1) 强制释放
				在键值上做文章，存入的是 current UNIX time+lock timeout+1，这样其他线程可以通过锁的键值对应的时间戳来判断这种情况是否发生了，如果当前的时间已经大于 lock.{orderid} 的键值，说明该锁已失效，可以被重新使用。
			2) 释放自己持有的锁时，先检查是否已超时
				持有锁的线程在解锁之前应该再检查一次自己的锁是否已经超时，再去做 DELETE 操作，因为可能客户端因为某个耗时的操作而挂起，操作完的时候锁因为超时已经被别人获得，这时就不必解锁了。
	 
		继续解决上面的办法会引入新问题：
			如果多个线程检测到锁超时，都尝试去释放锁，那么就会出现竞态条件 (race condition) 。
			场景是：
				C0 操作超时了，但它还持有着锁，C1 和 C2 读取 lock.{orderid} 检查时间戳，先后发现超时了。
				C1 发送 delete lock.{orderid}，
				C1 发送 set lock.{orderid} 且成功。
				C2 发送 delete lock.{orderid}，
				C2 发送 set lock.{orderid} 且成功。
			这样，C1 和 C2 都认为自己拿到了锁。
			如果比较在意这种竞态条件，那么推荐使用基于 zookeeper 或 redis 的解决方案。
	
	基于 ZooKeeper 的分布式锁
		
		这主要得益于 ZooKeeper 为我们保证了数据的强一致性，即用户只要完全相信每时每刻，zk 集群中任意节点 (一个 zk server) 上的相同 znode 的数据一定是相同的。锁服务可以分为两类，一个是保持独占，另一个是控制时序。
		所谓保持独占，就是所有试图来获取这个锁的客户端，最终只有一个可以成功获得这把锁。通常的做法是把 zk 上的一个 znode 看作是一把锁，通过 create znode 的方式来实现。所有客户端都去创建 /distributed_lock 节点，最终成功创建的那个客户端也就拥有了这把锁。

		控制时序，就是所有试图获取这个锁的客户端，最终都是会被安排执行，只是有个全局时序。做法和上面基本类似，只是这里 /distributed_lock 已经预先存在，客户端在它下面创建临时有序节点 (这个可以通过节点的属性控制：CreateMode.EPHEMERAL_SEQUENTIAL 来指 定) 。zk 的父节点 (/distributed_lock) 维持一份 sequence，保证子节点创建的时序性，从而形成了每个客户端的全局时序。

		ZooKeeper 里实现分布式锁的基本逻辑：

			客户端调用 create() 方法创建名为“_locknode_/guid-lock-”的节点，需要注意的是，这里节点的创建类型需要设置为 EPHEMERAL_SEQUENTIAL。
			客户端调用 getChildren(“_locknode_”) 方法来获取所有已经创建的子节点，同时在这个节点上注册上子节点变更通知的 Watcher。
			客户端获取到所有子节点 path 之后，如果发现自己在步骤 1 中创建的节点是所有节点中序号最小的，那么就认为这个客户端获得了锁。
			如果在步骤 3 中发现自己并非是所有子节点中最小的，说明自己还没有获取到锁，就开始等待，直到下次子节点变更通知的时候，再进行子节点的获取，判断是否获取锁。
			释放锁的过程相对比较简单，就是删除自己创建的那个子节点即可。

	 
	基于 Redis 的分布式锁
		
		接着前面的竞态条件说，同样的场景下，使用 Redis 的 SETNX (即 SET if Not eXists，类似于 memcache 的 add) 
		和 GETSET (先写新值，返回旧值，原子性操作，可以用于分辨是不是首次操作) 命令便可迎刃而解：
			C3 发送 SETNX lock.{orderid} 想要获得锁，由于 C0 还持有锁，所以 Redis 返回给 C3 一个 0，
			C3 发送 GET lock.{orderid} 以检查锁是否超时了，如果没超时，则等待或重试。
			反之，如果已超时，C3 通过下面的操作来尝试获得锁：
			GETSET lock.{orderid} <current Unix time + lock timeout + 1>
			通过 GETSET，C3 拿到的时间戳如果仍然是超时的，那就说明，C3 如愿以偿拿到锁了。
		如果在 C3 之前，有个叫 C4 的客户端比 C3 快一步执行了上面的操作，那么 C3 拿到的时间戳是个未超时的值，这时，C3 没有如期获得锁，需要再次等待或重试。留意一下，尽管 C3 没拿到锁，但它改写了 C4 设置的锁的超时值，不过这一点非常微小的误差带来的影响可以忽略不计。
		jeffkit 的伪码参考：
			# get lock
			lock = 0
			while lock != 1:
			 timestamp = current Unix time + lock timeout + 1
			 lock = SETNX lock.orderid timestamp
			 if lock == 1 or (now() > (GET lock.orderid) and now() > (GETSET lock.orderid timestamp)):
				break
			 else:
				sleep(10ms)
			 
			do_your_job()
			 
			# release lock
			if now() < GET lock.orderid:
			 DEL lock.orderid
		 


