# Architecture

---

## 场景设计

---

### 短链服务怎么设计

短链服务的基本逻辑：

> 1、将长链接变为短链；
>
> 2、用户访问短链接，会跳转到正确的长链接上去。即根据 db 里面里面找到信息找

短链生成方法：

> 短码一般是由 [a - z, A - Z, 0 - 9] 这 62 个字母或数字组成，短码的长度也可以自定义，但一般不超过 8 位。
>
> 比较常用的都是 6 位，6 位的短码已经能有 568 亿种的组合：(26+26+10)^6 = 56800235584，已满足绝大多数的使用场景。

目前比较流行的生成短码方法有：发号器算法 / 自增 id、哈希算法 / 摘要算法、普通随机数。

**发号器算法：**

发号器里面的号即为一个数字 ID

这里其实换一种思路，不要用长链作为入参，可以把一个唯一 ID 作为入参

- 如何选择唯一 ID:

> 1、比如 mysql 自增主键
>
> 2、前面自己写的文章 ID 分布式生成器

- 映射算法：

> 1、可参考 base62 编码,即 [0-9a-zA-Z] 字符集，能把唯一的 ID 换成 62 进制的编码，也会是唯一的

**哈希算法：**

以 MD5 算法为例，把长链直接 MD5 得到 32 位字符串，再通过移位等操作能拿到短链

但是这里类似 hash 会有一个问题是，即不同的长链，有能拿到同样的短链，这个时候需要解决冲突

原长连接加上随机字符串再重试，反复重试直到短链没有用过，即没有产生冲突

### 动态怎么设计，比如微信朋友圈或微博

核心：数据库 + 缓存。

进阶：推送 + 拉取 + 客户端做过滤和排序。

> MySQL：
>
> 我在专栏文章「微博技术解密（上） | 微博信息流是如何实现的？」讲到微博 Feed 的存储使用了两层的结构。为了减少对 MySQL 数据库的访问压力，在前面部署了 Memcached 缓存，挡住了 99% 的访问压力，只有 1% 的请求会访问数据库。
>
> 然而对于微博业务来说，这 1% 的请求也有几万 QPS，对于单机只能扛几千 QPS 的 MySQL 数据库来说还是太大了。为此我们又对数据库端口进行了拆分，你可以看下面的示意图，每个用户的 UID 是唯一的，不同 UID 的用户按照一定的 Hash 规则访问不同的端口，这样的话单个数据库端口的访问量就会变成原来的 1/8。
>
> 除此之外，考虑到微博的读请求量要远大于写请求量，所以有必要对数据库的读写请求进行分离，写请求访问 Master，读请求访问 Slave，这样的话 Master 只需要一套，Slave 根据访问量的需要可以有多套，也就是 “一主多从” 的架构。最后考虑到灾备的需要，还会在异地部署一套冷备的灾备数据库，平时不对外提供线上服务，每天对所有最新的数据进行备份，以防线上数据库发生同时宕机的情况。
>
> Memcached：
>
> 在 MySQL 数据库前面，还使用了 Memcached 作为缓存来承担几百万 QPS 的数据请求，产生的带宽问题是最大挑战。为此微博采用了下图所示的多层缓存结构，即 L1-Master-Slave，它们的作用各不相同。
>
> L1 主要起到分担缓存带宽压力的作用，并且如果有需要可以无限进行横向扩展，任何一次数据请求，都随机请求其中一组 L1 缓存，这样的话，假如一共 10 组 L1，数据请求量是 200 万 QPS，那么每一组 L1 缓存的请求量就是 1/10，也就是 20 万 QPS；同时每一组缓存又包含了 4 台机器，按照用户 UID 进行 Hash，每一台机器只存储其中一部分数据，这样的话每一台机器的访问量就只有 1/4 了。
>
> Master 主要起到防止访问穿透到数据库的作用，所以一般内存大小要比 L1 大得多，以存储尽可能多的数据。当 L1 缓存没有命中时，不能直接穿透到数据库，而是先访问 Master。
>
> Slave 主要起到高可用的目的，以防止 Master 的缓存宕机时，从 L1 穿透访问的数据直接请求数据库，起到 “兜底” 的作用。

### RPC 系统怎么设计，有哪些关键地方

1、通信协议：采用何种协议进行通信

2、序列化和反序列化的方式。既要考虑序列化 IO 的性能，又要考虑后续的兼容和维护。

3、服务注册和发现。

4、统一网关。

5、负载均衡：服务端负载均衡、客户端负载均衡。

### 秒杀怎么设计

从以下几个方面入手：

1、页面静态化：

用户浏览商品等常规操作，并不会请求到服务端。只有到了秒杀时间点，并且用户主动点了秒杀按钮才允许访问服务端。

2、CDN 加速：

使用 CDN，它的全称是 Content Delivery Network，即内容分发网络。

使用户就近获取所需内容，降低网络拥塞，提高用户访问响应速度和命中率。

3、缓存：

如果有数十万的请求过来，同时通过数据库查缓存是否足够，此时数据库可能会挂掉。因为数据库的连接资源非常有限，比如：mysql，无法同时支持这么多的连接。

而应该改用缓存，比如：redis。

即便用了 redis，也需要部署多个节点。

4、分布式锁：

如果在高并发下，有大量的请求都去查一个缓存中不存在的商品，这些请求都会直接打到数据库。数据库由于承受不住压力，而直接挂掉。

比如 redisson。

5、MQ 异步处理：

真正并发量大的是秒杀功能，下单和支付功能实际并发量很小。所以，我们在设计秒杀系统时，有必要把下单和支付功能从秒杀的主流程中拆分出来，特别是下单功能要做成 mq
异步处理的。而支付功能，比如支付宝支付，是业务场景本身保证的异步。

6、限流：

前端限制单个用户的请求频率。

服务端也需要加些对应控制，不能信任客户端的任何操作。

7、防止提前下单

防止提前下单主要是在静态化页面中加入一个 JS 文件引用，该 JS 文件包含活动是否开始的标记以及开始时的动态下单页面的 URL 参数。同时，这个 JS 文件是不会被 CDN
系统缓存的，会一直请求后端服务的，所以这个 JS 文件一定要很小。当活动快开始的时候（比如提前），通过后台接口修改这个 JS 文件使之生效

### 秒杀系统如何防止超卖

1、利用分布式锁

用 redis 分布式锁来做保证扣库存数量准确的环节，让点击结算时，后端逻辑会查询库存和扣库存的 update 语句同时只有一条线程能够执行，以商品 id 为分布式锁的 key，锁一个商品。

但是其他购买相同商品的用户将会进行等待。

2、利用分布式锁 + 分段缓存

借鉴 ConcurrenthashMap，分段锁的机制，把 100 个商品，分在 3 个段上，key 为分段名字，value 为库存数量。

用户下单时对用户 id 进行 %3 计算，看落在哪个 redis 的 key 上，就去取哪个。

方案有点复杂。

3、redis 的 lpush rpop

4、推荐使用 redis 原子操作

利用 redis 的 incr、decr 的原子性。

获取到后把数值填入 redis，以商品 id 为 key，数量为 value。

注意要设置序列化方式为 StringRedisSerializer，不然不能把 value 做加减操作。

还需要设置 redis 对应这个 key 的超时时间，以防所有商品库存数据都在 redis 中。

虽然 redis 已经防止了超卖，但是数据库层面，为了也要防止超卖，以防 redis 崩溃时无法使用或者不需要 redis 处理时，则用乐观锁，因为不一定全部商品都用 redis。

LUA 脚本保持库存原子性。用 lua 脚本先查询数量剩余多少，是否够减 100 后，再去减 100。

5、还要防止用户重复提交：

利用 redis 分布式锁，防止同一个用户同一秒里面把购物车里的商品进行多次结算，防止前端代码出问题触发两次。

---


---

## 场景优化

---

### 大流量场景下，服务撑不住了，可以怎么优化

**找性能瓶颈，解决对应问题。**

1、如果是数据库压力大，可以通过优化数据库来提升性能，

- 架构优化：读写分离，分库分表；
- 索引优化：索引覆盖；联合索引。
- SQL 优化：比如尽量不用 like；不要用 select *，不要返回用不到的字段。

2、如果没有使用缓存，可以加缓存，减少对数据库的资源占用。

3、如果是应用服务器资源占用过高，可以通过加服务器做负载均衡。

4、如果是内存占用过高：

- JVM 优化，堆内存等参数的设置；
- Java 程序优化，比如减少不必要的对象创建；
- 有条件还可以找运维扩容。

5、如果是服务器 cpu 占用过高：可以控制大文件的下载，减轻 cpu 负担。

6、前端方面：

- CDN 加锁；
- 网页的静态化；
- js、css 文件合并，都能减轻服务器的负担。

7、如果是临时流量突然飙升，限流是一个有效的方案。

8、消息队列 MQ 的合适使用，也能起到削峰的作用。





---

参考链接：

- [秒杀系统如何设计？](https://cloud.tencent.com/developer/article/1863530)
- [秒杀系统设计](https://www.jianshu.com/p/8b548419b4d3)
- [如何实现一个短链接服务](https://www.cnblogs.com/rickiyang/p/12178644.html)
- [短链服务](https://www.jianshu.com/p/3d8403071445)
- [新浪微博数据库是如何设计的？](https://www.zhihu.com/question/19715683)
- [大流量，高并发解决方案](https://zhuanlan.zhihu.com/p/370333338)
- [网站高并发大流量访问的处理及解决方法](https://cloud.tencent.com/developer/article/1102797)
- [高并发场景 - 订单库存防止超卖](https://segmentfault.com/a/1190000022596853)
- []()

---







